{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12735612,"sourceType":"datasetVersion","datasetId":8012130}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T07:03:58.016948Z","iopub.execute_input":"2025-08-20T07:03:58.017128Z","iopub.status.idle":"2025-08-20T07:03:58.320103Z","shell.execute_reply.started":"2025-08-20T07:03:58.017112Z","shell.execute_reply":"2025-08-20T07:03:58.319338Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_dev_test.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_dev.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1C_dev_test.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1C_dev.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_train.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1C_train.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# 0. Reproducibility\n# =========================  \nimport os, random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\nprint(\"‚úÖ Seeds set\")\n\n# =========================\n# 1. Paths & Config\n# =========================\ntrain_path = \"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1C_train.tsv\"\ndev_path   = \"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1C_dev.tsv\"\ntest_path  = \"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1C_dev_test.tsv\"\n\nMODEL_NAME = \"csebuetnlp/banglabert\"\nMAX_LEN = 256\nBATCH_SIZE = 8\nEPOCHS = 5\nLR = 2e-5\nMODEL_TAG = \"BanglaBERT_CSEBUET\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"üñ•Ô∏è Device:\", device)\n\n# =========================\n# 2. Load datasets\n# =========================\ntrain_df = pd.read_csv(train_path, sep=\"\\t\")\ndev_df   = pd.read_csv(dev_path,   sep=\"\\t\")\ntest_df  = pd.read_csv(test_path,  sep=\"\\t\")\n\nprint(\"üìÇ Train shape:\", train_df.shape)\nprint(\"üìÇ Dev shape  :\", dev_df.shape)\nprint(\"üìÇ Test shape :\", test_df.shape)\nprint(\"üîπ Sample train row:\\n\", train_df.head(1))\n\n# =========================\n# 3. Encode labels\n# =========================\nle_type = LabelEncoder()\nle_severity = LabelEncoder()\nle_whom = LabelEncoder()\n\ntrain_df[\"hate_type\"]      = le_type.fit_transform(train_df[\"hate_type\"])\ntrain_df[\"hate_severity\"]  = le_severity.fit_transform(train_df[\"hate_severity\"])\ntrain_df[\"to_whom\"]        = le_whom.fit_transform(train_df[\"to_whom\"])\n\ndev_df[\"hate_type\"]        = le_type.transform(dev_df[\"hate_type\"])\ndev_df[\"hate_severity\"]    = le_severity.transform(dev_df[\"hate_severity\"])\ndev_df[\"to_whom\"]          = le_whom.transform(dev_df[\"to_whom\"])\n\nprint(\"‚úÖ Labels encoded\")\nprint(\"  hate_type classes    :\", list(le_type.classes_))\nprint(\"  hate_severity classes:\", list(le_severity.classes_))\nprint(\"  to_whom classes      :\", list(le_whom.classes_))\n\n# =========================\n# 4. Tokenizer\n# =========================\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(\"‚úÖ Tokenizer loaded:\", MODEL_NAME)\n\n# =========================\n# 5. Dataset\n# =========================\nclass HateSpeechDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=64, is_test=False):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = str(row[\"text\"])\n\n        enc = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        item = {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n        }\n\n        if self.is_test:\n            item[\"id\"] = int(row[\"id\"])\n        else:\n            item[\"hate_type\"]     = torch.tensor(int(row[\"hate_type\"]), dtype=torch.long)\n            item[\"hate_severity\"] = torch.tensor(int(row[\"hate_severity\"]), dtype=torch.long)\n            item[\"to_whom\"]       = torch.tensor(int(row[\"to_whom\"]), dtype=torch.long)\n\n        return item\n\n# =========================\n# 6. Datasets & Loaders\n# =========================\ntrain_dataset = HateSpeechDataset(train_df, tokenizer, max_len=MAX_LEN, is_test=False)\ndev_dataset   = HateSpeechDataset(dev_df,   tokenizer, max_len=MAX_LEN, is_test=False)\ntest_dataset  = HateSpeechDataset(test_df,  tokenizer, max_len=MAX_LEN, is_test=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE, shuffle=False)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n\nprint(\"‚úÖ DataLoaders ready\")\n\n# =========================\n# 7. Model\n# =========================\nclass MultiTaskBERT(nn.Module):\n    def __init__(self, model_name, num_type, num_severity, num_whom, dropout=0.45):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(dropout)\n        hidden = self.encoder.config.hidden_size\n\n        self.type_head     = nn.Linear(hidden, num_type)\n        self.severity_head = nn.Linear(hidden, num_severity)\n        self.whom_head     = nn.Linear(hidden, num_whom)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n\n        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n            pooled = out.pooler_output\n        else:\n            pooled = out.last_hidden_state[:, 0, :]\n\n        pooled = self.dropout(pooled)\n\n        return (\n            self.type_head(pooled),\n            self.severity_head(pooled),\n            self.whom_head(pooled),\n        )\n\nmodel = MultiTaskBERT(\n    MODEL_NAME,\n    num_type=len(le_type.classes_),\n    num_severity=len(le_severity.classes_),\n    num_whom=len(le_whom.classes_),\n).to(device)\n\noptimizer = AdamW(model.parameters(), lr=LR)\nloss_fn = nn.CrossEntropyLoss()\nprint(\"‚úÖ Model & Optimizer ready\")\n\n# =========================\n# 8. Training\n# =========================\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0.0\n    for step, batch in enumerate(train_loader, start=1):\n        optimizer.zero_grad(set_to_none=True)\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        y_type = batch[\"hate_type\"].to(device)\n        y_sev  = batch[\"hate_severity\"].to(device)\n        y_whom = batch[\"to_whom\"].to(device)\n\n        logits_type, logits_sev, logits_whom = model(input_ids, attention_mask)\n\n        loss = (loss_fn(logits_type, y_type) +\n                loss_fn(logits_sev,  y_sev)  +\n                loss_fn(logits_whom, y_whom)) / 3.0\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        if step % 100 == 0:\n            print(f\"Epoch {epoch} | Step {step} | Batch Loss: {loss.item():.4f}\")\n\n    avg_loss = total_loss / max(1, len(train_loader))\n    print(f\"üìâ Epoch {epoch}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n\n# =========================\n# 9. Evaluation on Dev\n# =========================\nmodel.eval()\ny_true_type, y_pred_type = [], []\ny_true_sev,  y_pred_sev  = [], []\ny_true_whom, y_pred_whom = [], []\n\nwith torch.no_grad():\n    for batch in dev_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        logits_type, logits_sev, logits_whom = model(input_ids, attention_mask)\n\n        y_true_type.extend(batch[\"hate_type\"].cpu().numpy())\n        y_true_sev.extend(batch[\"hate_severity\"].cpu().numpy())\n        y_true_whom.extend(batch[\"to_whom\"].cpu().numpy())\n\n        y_pred_type.extend(logits_type.argmax(dim=1).cpu().numpy())\n        y_pred_sev.extend(logits_sev.argmax(dim=1).cpu().numpy())\n        y_pred_whom.extend(logits_whom.argmax(dim=1).cpu().numpy())\n\nf1_type = f1_score(y_true_type, y_pred_type, average=\"micro\")\nf1_severity = f1_score(y_true_sev,  y_pred_sev,  average=\"micro\")\nf1_whom = f1_score(y_true_whom, y_pred_whom, average=\"micro\")\nfinal_f1 = (f1_type + f1_severity + f1_whom) / 3.0\nprint(f\"üìä Dev Avg Micro-F1: {final_f1:.4f} \"\n      f\"(type={f1_type:.4f}, severity={f1_severity:.4f}, whom={f1_whom:.4f})\")\n\n# =========================\n# 10. Predict for Submission\n# =========================\npreds_type, preds_sev, preds_whom, ids = [], [], [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        logits_type, logits_sev, logits_whom = model(input_ids, attention_mask)\n\n        preds_type.extend(logits_type.argmax(dim=1).cpu().numpy().tolist())\n        preds_sev.extend(logits_sev.argmax(dim=1).cpu().numpy().tolist())\n        preds_whom.extend(logits_whom.argmax(dim=1).cpu().numpy().tolist())\n\n        if isinstance(batch[\"id\"], torch.Tensor):\n            ids.extend(batch[\"id\"].cpu().numpy().astype(int).tolist())\n        else:\n            ids.extend([int(x) for x in batch[\"id\"]])\n\n# Decode back to original labels\npreds_type = le_type.inverse_transform(np.array(preds_type))\npreds_sev  = le_severity.inverse_transform(np.array(preds_sev))\npreds_whom = le_whom.inverse_transform(np.array(preds_whom))\n\n# Create submission DataFrame\nsubmission = pd.DataFrame({\n    \"id\": ids,\n    \"hate_type\": preds_type,\n    \"hate_severity\": preds_sev,\n    \"to_whom\": preds_whom,\n    \"model\": [MODEL_TAG] * len(ids)\n})\n\n# ‚úÖ Fix invalid values (Codabench checker strict)\nsubmission[\"hate_type\"] = submission[\"hate_type\"].fillna(\"None\")\nsubmission[\"hate_severity\"] = submission[\"hate_severity\"].fillna(\"Little to None\")\nsubmission[\"to_whom\"] = submission[\"to_whom\"].fillna(\"None\")\n\n# Strip accidental spaces\nfor col in [\"hate_type\", \"hate_severity\", \"to_whom\"]:\n    submission[col] = submission[col].astype(str).str.strip()\n\n# Save with header (checker skips header itself)\nsubmission.to_csv(\"submission.tsv\", sep=\"\\t\", index=False)\nprint(\"‚úÖ Submission file saved as submission.tsv\")\nprint(submission.head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T07:03:58.321685Z","iopub.execute_input":"2025-08-20T07:03:58.322013Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Seeds set\nüñ•Ô∏è Device: cuda\nüìÇ Train shape: (35522, 5)\nüìÇ Dev shape  : (2512, 5)\nüìÇ Test shape : (2512, 2)\nüîπ Sample train row:\n        id                                               text hate_type  \\\n0  147963  ‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶ ‡¶¨‡¶∞‡ßç‡¶°‡¶æ‡¶∞ ‡¶ó‡¶æ‡¶∞‡ßç‡¶° ‡¶¶‡ßá‡¶∞‡¶ï‡ßá ‡¶è‡¶≠‡¶æ‡¶¨‡ßá ‡¶™‡¶æ‡¶π‡¶æ‡¶∞‡¶æ ‡¶¶‡¶ø‡¶§‡ßá ‡¶π...       NaN   \n\n    hate_severity to_whom  \n0  Little to None     NaN  \n‚úÖ Labels encoded\n  hate_type classes    : ['Abusive', 'Political Hate', 'Profane', 'Religious Hate', 'Sexism', nan]\n  hate_severity classes: ['Little to None', 'Mild', 'Severe']\n  to_whom classes      : ['Community', 'Individual', 'Organization', 'Society', nan]\n‚úÖ Tokenizer loaded: csebuetnlp/banglabert\n‚úÖ DataLoaders ready\n","output_type":"stream"},{"name":"stderr","text":"2025-08-20 07:04:06.159950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755673446.181719      88 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755673446.188341      88 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ce9cdf6d3749eaae29a90499c6758c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d5e4ffb9cf4930b1cfce8f64de0549"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Model & Optimizer ready\nEpoch 1 | Step 100 | Batch Loss: 0.9671\nEpoch 1 | Step 200 | Batch Loss: 0.8617\nEpoch 1 | Step 300 | Batch Loss: 0.4732\nEpoch 1 | Step 400 | Batch Loss: 0.9671\nEpoch 1 | Step 500 | Batch Loss: 0.6258\nEpoch 1 | Step 600 | Batch Loss: 1.1698\nEpoch 1 | Step 700 | Batch Loss: 1.0837\nEpoch 1 | Step 800 | Batch Loss: 1.2145\n","output_type":"stream"}],"execution_count":null}]}