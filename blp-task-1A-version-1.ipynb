{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12678630,"sourceType":"datasetVersion","datasetId":8012130}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-10T16:00:11.796384Z","iopub.execute_input":"2025-08-10T16:00:11.796591Z","iopub.status.idle":"2025-08-10T16:00:13.991232Z","shell.execute_reply.started":"2025-08-10T16:00:11.796573Z","shell.execute_reply":"2025-08-10T16:00:13.990467Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_dev_test.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_dev.tsv\n/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_train.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# প্রয়োজনীয় প্যাকেজ (Kaggle সেশনে চালাও)\n!pip install -q transformers accelerate datasets peft bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T16:00:13.992122Z","iopub.execute_input":"2025-08-10T16:00:13.992550Z","iopub.status.idle":"2025-08-10T16:01:37.174338Z","shell.execute_reply.started":"2025-08-10T16:00:13.992521Z","shell.execute_reply":"2025-08-10T16:01:37.173435Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nqwen3_1.7b_lora_finetune_and_submit.py\n\nFull pipeline using unsloth/Qwen3-1.7B:\n- LoRA (PEFT) fine-tune on train/dev\n- Save adapter\n- Few-shot inference on dev_test\n- Save submission TSV (id, label, model)\n\n**WARNING**: Qwen3-1.7b is large. Ensure your GPU has enough memory (>=~48GB recommended).\nIf OOM: switch MODEL_NAME to a smaller model or reduce MAX_LEN/BATCH_SIZE/LORA_R.\n\"\"\"\n\nimport os\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n\n# ---------------------------\n# Config (edit if needed)\n# ---------------------------\nTRAIN_PATH = \"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_train.tsv\"\nDEV_PATH = \"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_dev.tsv\"\nDEV_TEST_PATH = \"/kaggle/input/blp25-task1/blp25_hatespeech_subtask_1A_dev_test.tsv\"\n\nMODEL_NAME = \"unsloth/Qwen3-1.7B\"       # requested model\nOUTPUT_DIR = \"./qwen3_1.7b_lora_adapter\"\nOUTPUT_SUBMISSION = \"submission_qwen3_1.7b_lora.tsv\"\n\n# Training hyperparams (tune to your hardware)\nMAX_LEN = 256            # prompt + label max tokens\nBATCH_SIZE = 1           # per-device batch size (1 recommended for large models)\nEPOCHS = 2               # number of epochs (keep small)\nLEARNING_RATE = 2e-4\nLORA_R = 8               # reduce to 4 if OOM\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.05\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nLABEL_LIST = [\"Abusive\", \"Sexism\", \"Religious Hate\", \"Political Hate\", \"Profane\", \"None\"]\n\nprint(f\"Device: {DEVICE}  Model: {MODEL_NAME}\")\n\n# ---------------------------\n# Helpers\n# ---------------------------\ndef make_prompt_base(text):\n    return (\n        \"Instruction: Classify the following Bangla YouTube comment into one of \"\n        f\"{LABEL_LIST}.\\n\\nComment:\\n{text}\\n\\nLabel:\"\n    )\n\ndef load_tsv(path):\n    return pd.read_csv(path, sep=\"\\t\", quoting=3, engine=\"python\")\n\n# ---------------------------\n# Load datasets\n# ---------------------------\ntrain_df = load_tsv(TRAIN_PATH)\ndev_df = load_tsv(DEV_PATH)\ndev_test_df = load_tsv(DEV_TEST_PATH)\n\nprint(\"Sizes -> train:\", len(train_df), \"dev:\", len(dev_df), \"dev_test:\", len(dev_test_df))\n\n# ---------------------------\n# Tokenizer\n# ---------------------------\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# ---------------------------\n# Encode for causal LM training\n# ---------------------------\ndef encode_full(prompt_text, label_text, max_len=MAX_LEN):\n    full = prompt_text + \" \" + label_text\n    enc_full = tokenizer(full, truncation=True, max_length=max_len, padding=\"max_length\")\n    prompt_ids = tokenizer(prompt_text, truncation=True, max_length=max_len)[\"input_ids\"]\n    prompt_len = len(prompt_ids)\n    labels = [-100] * prompt_len + enc_full[\"input_ids\"][prompt_len:]\n    labels = labels[:max_len] + [-100] * max(0, max_len - len(labels))\n    return enc_full[\"input_ids\"], enc_full[\"attention_mask\"], labels\n\ndef build_examples(df, include_label=True):\n    examples = []\n    for _, row in df.iterrows():\n        text = str(row[\"text\"])\n        prompt_text = make_prompt_base(text)\n        if include_label:\n            label_text = str(row[\"label\"])\n            input_ids, att_mask, labels = encode_full(prompt_text, label_text)\n            examples.append({\"input_ids\": input_ids, \"attention_mask\": att_mask, \"labels\": labels})\n        else:\n            enc = tokenizer(prompt_text, truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n            examples.append({\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]})\n    return examples\n\nprint(\"Encoding datasets (this can be slow)...\")\ntrain_examples = build_examples(train_df, include_label=True)\ndev_examples = build_examples(dev_df, include_label=True)\ndev_test_examples = build_examples(dev_test_df, include_label=False)\n\ntrain_ds = Dataset.from_list(train_examples).with_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\ndev_ds = Dataset.from_list(dev_examples).with_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\ndev_test_ds = Dataset.from_list(dev_test_examples).with_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n\n# ---------------------------\n# Load base model\n# ---------------------------\nprint(\"Loading base model (attempt 8-bit to save memory)...\")\nbase_model = None\ntry:\n    base_model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        load_in_8bit=True,    # requires bitsandbytes\n        device_map=\"auto\"\n    )\n    print(\"Loaded base model in 8-bit.\")\n    try:\n        base_model = prepare_model_for_kbit_training(base_model)\n    except Exception:\n        pass\nexcept Exception as e:\n    print(\"8-bit attempt failed:\", e)\n    print(\"Falling back to fp16 load with device_map auto.\")\n    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\n\n# ---------------------------\n# Apply LoRA (PEFT)\n# ---------------------------\nprint(\"Applying LoRA adapter (PEFT)...\")\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(base_model, lora_config)\nprint(\"LoRA applied. Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n\n# ---------------------------\n# Trainer setup\n# ---------------------------\ndef collate_fn(batch):\n    import torch\n    input_ids = torch.tensor([b[\"input_ids\"] for b in batch], dtype=torch.long)\n    attention_mask = torch.tensor([b[\"attention_mask\"] for b in batch], dtype=torch.long)\n    if \"labels\" in batch[0]:\n        labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n    else:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen3_1.7b_lora_training\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=1,\n    num_train_epochs=EPOCHS,\n    learning_rate=LEARNING_RATE,\n    fp16=torch.cuda.is_available(),\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=50,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=dev_ds,\n    data_collator=collate_fn,\n    tokenizer=tokenizer\n)\n\n# ---------------------------\n# Train LoRA adapter\n# ---------------------------\nprint(\"Starting LoRA fine-tuning (monitor for OOM)...\")\ntry:\n    trainer.train()\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    model.save_pretrained(OUTPUT_DIR)\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    print(\"Saved LoRA adapter to:\", os.path.abspath(OUTPUT_DIR))\nexcept Exception as e:\n    print(\"Training failed or interrupted:\", e)\n    print(\"If OOM: try MAX_LEN=128, BATCH_SIZE=1, LORA_R=4 or use smaller MODEL_NAME.\")\n    # attempt to save adapter if possible\n    try:\n        model.save_pretrained(OUTPUT_DIR)\n        tokenizer.save_pretrained(OUTPUT_DIR)\n        print(\"Adapter saved to:\", os.path.abspath(OUTPUT_DIR))\n    except Exception as e2:\n        print(\"Could not save adapter:\", e2)\n\n# ---------------------------\n# Few-shot examples (customize if desired)\n# ---------------------------\nfew_shot_examples = [\n    (\"ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছে এক নদীতে দুইনীতি কেমনে হয়\", \"Political Hate\"),\n    (\"লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানসিকতার মানুষ গুলো সরকারি চাকরিতে কাজ করেন\", \"Abusive\"),\n    (\"শালার ব্যাটা খুব বকা দিতে পারে ইচ্ছা করে না নির্লজ্জ\", \"Profane\"),\n    (\"হামাস তুমি সারাজীবন ইসলামের দৃষ্টিতে থাকবে\", \"None\"),\n    (\"মহিলা দিয়ে দেশ চালালে এর থেকে বেশি কি আশা করা যায়\", \"Sexism\"),\n    (\"মুসলিম মুসলিম যুদ্ধ করে কেনো\", \"Religious Hate\"),\n]\n\ndef make_few_shot_prompt(text):\n    prompt = (\n        f\"Instruction: Classify the following Bangla YouTube comment into one of {LABEL_LIST}.\\n\\n\"\n        \"Here are some examples:\\n\\n\"\n    )\n    for ex_text, ex_label in few_shot_examples:\n        prompt += f\"Comment: {ex_text}\\nLabel: {ex_label}\\n\\n\"\n    prompt += f\"Comment: {text}\\nLabel:\"\n    return prompt\n\n# ---------------------------\n# Inference: load base + adapter and predict on dev_test\n# ---------------------------\nprint(\"Loading base model for inference and attaching adapter...\")\n# load base model for inference (fp16)\nbase_for_infer = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\")\nadapter_path = os.path.abspath(OUTPUT_DIR)\nif not os.path.isdir(adapter_path):\n    raise FileNotFoundError(f\"Adapter path not found: {adapter_path}. Ensure training saved the adapter.\")\n\nmodel_peft = PeftModel.from_pretrained(base_for_infer, adapter_path)\nmodel_peft.eval()\n\n# tokenizer from adapter (so special tokens preserved)\ntokenizer = AutoTokenizer.from_pretrained(adapter_path if os.path.isdir(adapter_path) else MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Generation config\ngen_conf = GenerationConfig(max_new_tokens=12, do_sample=False) \n\n# Batch inference\nprint(\"Running few-shot inference on dev_test...\")\npreds = []\nbatch_size_inf = 4 if torch.cuda.is_available() else 1\nfor i in tqdm(range(0, len(dev_test_df), batch_size_inf)):\n    batch_texts = dev_test_df[\"text\"].iloc[i:i+batch_size_inf].tolist()\n    batch_prompts = [make_few_shot_prompt(t) for t in batch_texts]\n    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN).to(model_peft.device)\n    with torch.no_grad():\n        outs = model_peft.generate(**inputs, generation_config=gen_conf)\n    decs = tokenizer.batch_decode(outs, skip_special_tokens=True)\n    for txt in decs:\n        if \"Label:\" in txt:\n            pred = txt.split(\"Label:\")[-1].strip().split()[0]\n        else:\n            pred = txt.strip().split()[-1]\n        if pred not in LABEL_LIST:\n            pred = \"None\"\n        preds.append(pred)\n\n# pad if needed\nwhile len(preds) < len(dev_test_df):\n    preds.append(\"None\")\n\n# Save submission\nsubmission_df = pd.DataFrame({\"id\": dev_test_df[\"id\"], \"label\": preds, \"model\": \"qwen3_1.7b_lora_fewshot\"})\nsubmission_df.to_csv(OUTPUT_SUBMISSION, sep=\"\\t\", index=False)\nprint(\"Saved submission to\", OUTPUT_SUBMISSION)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T16:16:05.797743Z","iopub.execute_input":"2025-08-10T16:16:05.798307Z","iopub.status.idle":"2025-08-10T16:26:58.081794Z","shell.execute_reply.started":"2025-08-10T16:16:05.798280Z","shell.execute_reply":"2025-08-10T16:26:58.081149Z"}},"outputs":[{"name":"stdout","text":"Device: cuda  Model: unsloth/Qwen3-1.7B\nSizes -> train: 35522 dev: 2512 dev_test: 2512\nLoading tokenizer...\nEncoding datasets (this can be slow)...\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"Loading base model (attempt 8-bit to save memory)...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2740309046.py:181: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Loaded base model in 8-bit.\nApplying LoRA adapter (PEFT)...\nLoRA applied. Trainable params: 1605632\nStarting LoRA fine-tuning (monitor for OOM)...\nTraining failed or interrupted: only integer tensors of a single element can be converted to an index\nIf OOM: try MAX_LEN=128, BATCH_SIZE=1, LORA_R=4 or use smaller MODEL_NAME.\nAdapter saved to: /kaggle/working/qwen3_1.7b_lora_adapter\nLoading base model for inference and attaching adapter...\nRunning few-shot inference on dev_test...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/628 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb26ab57ee840b5a61520384ffd812f"}},"metadata":{}},{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'max_length': 40960, 'do_sample': True, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95, 'pad_token_id': 151654, 'bos_token_id': 151643, 'eos_token_id': [151645, 151643]}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"Saved submission to submission_qwen3_1.7b_lora.tsv\n","output_type":"stream"}],"execution_count":4}]}